# 单任务版爬虫实现步骤
1. 获取初始页面内容

2. 解决乱码问题
    1. 从网页文本中发现其编码，发现编码有两种方式
        1. 直接去html中找``charset``查看它的值
        2. go语言中有专门支持自动检测编码的包``gopm get -g -v golang.org/x/net/html``
    2. 如果不是utf-8,则转换为golang支持的utf-8编码

3. 使用正则表达式获取城市名称和链接    

    1. 回顾正则表达式

    2. 提取城市和URL

       1. 提取出包含城市和URL的一行

       2. 提取出其中的城市和URL

          

4. 单任务版爬虫总体算法 

    1. 抽象解析器

        1. 输入:utf-8的文本
        2. 输出:Request{URL,对应的Parser}列表，Item(真正获得有价值的数据)列表

    2. 单机版的爬虫架构

    3. 单任务版爬虫思路

        1. 首先有一个Engine，将整个动作进行驱动
        2. seed代表种子(比如起始URL``http://www.zhenai.com/zhenghun``)
        3. 将种子和解析器包装一下，作为Request送给engine
        4. engine将request加载到任务队列，方便维护
        5. 从任务队列中取出任务(request),取出request中的URL
        6. 将URL送给Fetcher，Fethcer代表从网络上面获取数据的一个模块，主要负责网络连接，获取数据，文本格式的转换，所以这里Fetcher返回一个UTF-8的文本给Engine
        7. Engine获取到文本之后，转给Parser，然后Parser负责返回requests,items。将requests再加入到任务队列中，items列表，直接打印即可
            1. 需要注意的是这里的requests:比如从起始URL中获取到的文本就包含了每一个城市的URL，也就是请求的目标，所以返回的requests列表也是我们将来要执行的任务，所以要加入到任务队列中

    4. 单任务版爬虫具体实现

        1. 封装fetcher:新建fetcher目录与文件，将main里面的请求URL函数以及编码识别函数进行封装

        2. 封装Parser:添加zhenai/parser,在parser中添加城市列表解析器，注意：这里采用这种方式是为了后面也可以解析其它网站，不同的网站解析规则不同

        3. 实现一个城市列表解析函数，根据前面的架构：不管什么Parser返回的都是一个Request和一个Items，所以再添加一个目录**engine**,这些request和item的集合是在engine中进行处理，添加一个Request和一个解析结构ParseResult。结构如下

            

        4. 实现parserCityList函数,在目录zhenai/parser下添加clitylist.go，添加如下函数

            

        5. engine/types.go中添加一个NilParser函数，为了让上面的函数编译通过

            

        6. 实现engine启动函数(**engine/engine.go**)

            

        7. 实现parseCityList测试函数

            1. 第一步，将产生的数据打印出来，方便拷贝

                

            2. 第二步，从上面的打印结果，我们可以看到数据最开始产生了截断，修改fetcher/fetcher.go中的determineEncoding函数

                

        8. 实现城市解析器

            1. 先随机查看一下某个城市的html源码，找到要匹配的信息

            2. 实现城市解析器

                

            3. 修改城市列表解析器函数

                

        9. 实现用户信息解析功能

            1. 点击一个用户，查看其详细信息，找到属于当前用户的字段信息，进行正则匹配

            2. 封装用户信息结构体，封装到model中，注意，这里为什么不直接封装到zhenai中，因为作为一个项目，我们可能会获取到各个相亲网站的信息汇总到数据库中，这样更有价值

                

            3. 通过正则匹配从网站爬取结构体中的每一个字段相关信息

                1. 除Name的每个正则匹配

                

                1. 实现用户解析函数与公共函数

                

            4. 修改请求函数，模拟网页请求，防止403(**fetcher/fetcher.go**)

                

            5. 添加用户解析测试函数

                

            6. 优化用户名获取：根据前面的城市解析功能，其实我们是能够拿到用户名的，所以考虑直接传过来，不使用两次匹配的方式

                

            7. 为了测试逻辑效果，可以先添加读取限制，读取10个城市 ，修改cityList.go

                

            8. 通过测试我们发现名字是不对的，首先查找原因

                

            9. 上面的函数中，在做闭包时，只是给了parseFunc一个函数，但并未在此时调用。那应该在何处进行调用呢？在profile中调用，而等到这个时候m[2]的值早就已经变了，因为m[2]在整个matches的循环中起到作用，所以每一次都会修改它的值，那应该怎么修改？用一个局部变量来拷贝m[2]的值，然后再行传递，这样这个局部变量name的值每次变化都会重新分配，但它的值已经传到了函数中，所以下一次不会再影响它。

                


